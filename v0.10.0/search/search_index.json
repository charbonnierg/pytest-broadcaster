{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Pytest Broadcaster","text":"<p>A plugin to write pytest collect output to various destinations.</p> pip install pytest-broadcasterpytest --collect-report=report.json"},{"location":"LICENSE/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2025, Guillaume Charbonnier</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"SUMMARY/","title":"SUMMARY","text":"<ul> <li>Home</li> <li>Usage</li> <li>Why ?</li> <li>JSON Schemas</li> <li>Reference</li> <li>Recipes</li> <li>License</li> </ul>"},{"location":"why/","title":"Why another plugin ?","text":"<p>If you ever wanter to build a tool that needs to parse the output of <code>pytest</code>, you may have noticed that the output is not very easy to parse.</p> <p>The page Managing pytest's output describes the different output formats of <code>pytest</code>, and how to use them.</p>"},{"location":"why/#built-in-solutions","title":"Built-in solutions","text":"<p>The built-in solutions are:</p> <ul> <li> <p>Creating resultlog format files: <code>pytest --resultlog=path</code>. However, the format is not easy to parse, and this option has been removed in version 6.0.</p> </li> <li> <p>Creating JUnitXML format files: <code>pytest --junitxml=path</code>. This format is easier to parse, but does not contain all the information that <code>pytest</code> outputs. Also, I'm not aware of a XML schema for the output of <code>pytest</code>, and using user properties will break the validation schema used by some CI servers. Instead, it is recommended to use <code>record-xml-attribute</code>.</p> </li> </ul> <p>And that's it. There is no built-in solution to get a structured output of <code>pytest</code> other than JUnit that can be easily parsed by other tools.</p>"},{"location":"why/#plugins","title":"Plugins","text":"<p>There are plugins however, that can help you parse the output of <code>pytest</code>:</p> <ul> <li> <p><code>pytest-reportlog</code>: Replacement for the --resultlog option, focused in simplicity and extensibility.</p> </li> <li> <p><code>pytest-json-report</code>: A pytest plugin to report test results in JSON format.</p> </li> <li> <p><code>pytest-csv</code>: A pytest plugin to report test results in CSV format.</p> </li> </ul>"},{"location":"why/#shortcomings-of-plugins","title":"Shortcomings of plugins","text":"<p>The plugins above are great, but:</p> <ul> <li> <p><code>pytest-reportlog</code> is focused on test result and omits the collection phase. It also does not provide JSON schemas to parse the output in other languages.</p> </li> <li> <p><code>pytest-json-report</code> is powerful, but can only write to files, and does not provide JSON schemas to parse the output in other languages.</p> </li> <li> <p><code>pytest-csv</code> is also powerful, especially in reporting meaningful information, but it can be tedious to parse reports from other languages.</p> </li> </ul>"},{"location":"why/#pytest-broadcaster","title":"<code>pytest-broadcaster</code>","text":"<p>This plugin aims to provide a more structured output that can be easily parsed by other tools.</p> <p>It does not aim to limit users with a single output destination (e.g., JSON file, JSON Lines file, HTTP Webhook), but to provide a flexible way to output the data to various destinations.</p> <p>JSON schemas are also provided for clients to help them parse the output of the plugin.</p> <p>The fact that this plugin works in two phase:</p> <ul> <li>reporting</li> <li>emitting</li> </ul> <p>allows it to be more flexible in the way it can be used.</p> <p>The fact that it provides JSON Schema for the output allows it to generate code in other languages to parse the output of the plugin.</p>"},{"location":"recipes/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Code Generation</li> </ul>"},{"location":"recipes/code_generation/","title":"Code Generation","text":"<p>JSON Schemas are provided for the data models.</p> <p>They can be used to generate code in various languages:</p> Python DataclassesPython Pydantic (v2)Python Pydantic (v1)Typescript <code>.d.ts</code> <p>It' possible to generate Python dataclasses from the JSON Schemas using <code>datamodel-code-generator</code>:</p> <ol> <li>First clone the repository:</li> </ol> <p>git clone https://github.com/charbonnierg/pytest-broadcastercd pytest-broadcaster</p> <ol> <li>Then install <code>datamodel-code-generator</code>:</li> </ol> <p>pip install --user datamodel-code-generator</p> <ol> <li>Then generate the code:</li> </ol> <p>datamodel-codegen \\     --input docs/schemas/ \\     --output models/ \\     --input-file-type jsonschema \\     --disable-timestamp \\     --output-model-type=dataclasses.dataclass \\     --use-field-description \\     --use-schema-description</p> <p>The generated code will be in the <code>models</code> directory.</p> <p>It' possible to generate Pydantic BaseModel classes from the JSON Schemas using <code>datamodel-code-generator</code>:</p> <ol> <li>First clone the repository:</li> </ol> <p>git clone https://github.com/charbonnierg/pytest-broadcastercd pytest-broadcaster</p> <ol> <li>Then install <code>datamodel-code-generator</code>:</li> </ol> <p>pip install --user datamodel-code-generator</p> <ol> <li>Then generate the code:</li> </ol> <p>datamodel-codegen \\     --input docs/schemas/ \\     --output models/ \\     --input-file-type jsonschema \\     --disable-timestamp \\     --output-model-type=pydantic_v2.BaseModel \\     --use-field-description \\     --use-schema-description</p> <p>The generated code will be in the <code>models</code> directory.</p> <p>It' possible to generate Pydantic BaseModel classes from the JSON Schemas using <code>datamodel-code-generator</code>:</p> <ol> <li>First clone the repository:</li> </ol> <p>git clone https://github.com/charbonnierg/pytest-broadcastercd pytest-broadcaster</p> <ol> <li>Then install <code>datamodel-code-generator</code>:</li> </ol> <p>pip install --user datamodel-code-generator</p> <ol> <li>Then generate the code:</li> </ol> <p>datamodel-codegen \\     --input docs/schemas/ \\     --output models/ \\     --input-file-type jsonschema \\     --disable-timestamp \\     --output-model-type=pydantic_v1.BaseModel \\     --use-field-description \\     --use-schema-description</p> <p>The generated code will be in the <code>models/</code> directory.</p> <p>It's possible to generate <code>.d.ts</code> files for Typescript using <code>json-schema-to-typescript</code>:</p> <ol> <li>First clone the repository:</li> </ol> <p>git clone https://github.com/charbonnierg/pytest-broadcastercd pytest-broadcaster</p> <ol> <li>Then install <code>json-schema-to-typescript</code>:</li> </ol> <p>npm install -g json-schema-to-typescript</p> <ol> <li>Then generate the code:</li> </ol> <p>json2ts -i docs/schemas/ -o types/</p> <p>The generated code will be in the <code>types/</code> directory.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Events</li> <li>Result</li> <li>Interfaces</li> <li>Hooks</li> <li>Plugin Implementation</li> </ul>"},{"location":"reference/plugin/","title":"Plugin implementation","text":""},{"location":"reference/plugin/#pytest_broadcaster.plugin.pytest_addoption","title":"pytest_addoption","text":"<pre><code>pytest_addoption(parser: Parser) -&gt; None\n</code></pre> <p>Register argparse-style options and ini-style config values.</p> <p>This function is called once at the beginning of a test run.</p> <p>Performs the following action:</p> <ul> <li>Get or create the <code>terminal reporting</code> group in the parser.</li> <li>Add the <code>--collect-report</code> option to the group.</li> <li>Add the <code>--collect-log</code> option to the group.</li> <li>Add the <code>--collect-url</code> option to the group.</li> <li>Add the <code>--collect-log-url</code> option to the group.</li> </ul> <p>See pytest.hookspec.pytest_addoption.</p> Source code in <code>src/pytest_broadcaster/plugin.py</code> <pre><code>def pytest_addoption(parser: pytest.Parser) -&gt; None:\n    \"\"\"Register argparse-style options and ini-style config values.\n\n    This function is called once at the beginning of a test run.\n\n    Performs the following action:\n\n    - Get or create the `terminal reporting` group in the parser.\n    - Add the `--collect-report` option to the group.\n    - Add the `--collect-log` option to the group.\n    - Add the `--collect-url` option to the group.\n    - Add the `--collect-log-url` option to the group.\n\n    See [pytest.hookspec.pytest_addoption][_pytest.hookspec.pytest_addoption].\n    \"\"\"\n    group = parser.getgroup(\n        name=\"terminal reporting\",\n        description=\"pytest-broadcaster plugin options\",\n    )\n    group.addoption(\n        \"--collect-report\",\n        action=\"store\",\n        metavar=\"path\",\n        default=None,\n        help=\"Path to JSON output file holding collected items.\",\n    )\n    group.addoption(\n        \"--collect-log\",\n        action=\"store\",\n        metavar=\"path\",\n        default=None,\n        help=\"Path to JSON Lines output file where events are logged to.\",\n    )\n    group.addoption(\n        \"--collect-url\",\n        action=\"store\",\n        metavar=\"url\",\n        default=None,\n        help=\"URL to send collected items to.\",\n    )\n    group.addoption(\n        \"--collect-log-url\",\n        action=\"store\",\n        metavar=\"url\",\n        default=None,\n        help=\"URL to send events to.\",\n    )\n</code></pre>"},{"location":"reference/plugin/#pytest_broadcaster.plugin.pytest_configure","title":"pytest_configure","text":"<pre><code>pytest_configure(config: Config) -&gt; None\n</code></pre> <p>Perform initial plugin configuration.</p> <p>This function is called once after command line options have been parsed.</p> <p>Perform the following actions:</p> <ul> <li>Skip if workerinput is present, which means we are in a worker process.</li> <li>Create a JSONFile destination if the JSON output file path is present.</li> <li>Create a JSONLinesFile destination if the JSON Lines output file path is present.</li> <li>Create an HTTPWebhook destination if the URL is present.</li> <li>Create an HTTPWebhook destination if the URL for the JSON Lines output file is present.</li> <li>Let the user add their own destinations if they want to.</li> <li>Create the default reporter.</li> <li>Let the user set the reporter if they want to.</li> <li>Create, open and register the plugin instance.</li> <li>Store the plugin instance in the config object.</li> </ul> <p>See pytest.hookspec.pytest_configure.</p> Source code in <code>src/pytest_broadcaster/plugin.py</code> <pre><code>def pytest_configure(config: pytest.Config) -&gt; None:\n    \"\"\"Perform initial plugin configuration.\n\n    This function is called once after command line options have been parsed.\n\n    Perform the following actions:\n\n    - Skip if workerinput is present, which means we are in a worker process.\n    - Create a JSONFile destination if the JSON output file path is present.\n    - Create a JSONLinesFile destination if the JSON Lines output file path is present.\n    - Create an HTTPWebhook destination if the URL is present.\n    - Create an HTTPWebhook destination if the URL for the JSON Lines output file is present.\n    - Let the user add their own destinations if they want to.\n    - Create the default reporter.\n    - Let the user set the reporter if they want to.\n    - Create, open and register the plugin instance.\n    - Store the plugin instance in the config object.\n\n    See [pytest.hookspec.pytest_configure][_pytest.hookspec.pytest_configure].\n    \"\"\"\n    # Skip if pytest-xdist worker\n    if hasattr(config, \"workerinput\"):\n        return\n\n    # Create publishers\n    destinations: list[Destination] = []\n\n    if json_path := config.option.collect_report:\n        destinations.append(JSONFile(json_path))\n\n    if json_lines_path := config.option.collect_log:\n        destinations.append(JSONLinesFile(json_lines_path))\n\n    if json_url := config.option.collect_url:\n        destinations.append(HTTPWebhook(json_url, emit_events=False, emit_result=True))\n\n    if json_lines_url := config.option.collect_log_url:\n        destinations.append(\n            HTTPWebhook(json_lines_url, emit_events=True, emit_result=False)\n        )\n\n    def add_destination(destination: Destination) -&gt; None:\n        destinations.append(destination)\n\n    # Let the user add their own destinations if they want to\n    config.hook.pytest_broadcaster_add_destination(add=add_destination)\n\n    # Create default reporter\n    reporter_to_use: Reporter = DefaultReporter()\n\n    def set_reporter(reporter: Reporter) -&gt; None:\n        nonlocal reporter_to_use\n        reporter_to_use = reporter\n\n    # Let the user set the reporter if they want to\n    config.hook.pytest_broadcaster_set_reporter(set=set_reporter)\n\n    # Create plugin instance.\n    plugin = PytestBroadcasterPlugin(\n        config=config,\n        reporter=reporter_to_use,\n        publishers=destinations,\n    )\n    # Open the plugin\n    plugin.open()\n    # Register the plugin with the plugin manager.\n    config.pluginmanager.register(plugin)\n    setattr(config, __PLUGIN_ATTR__, plugin)\n</code></pre>"},{"location":"reference/plugin/#pytest_broadcaster.plugin.pytest_unconfigure","title":"pytest_unconfigure","text":"<pre><code>pytest_unconfigure(config: Config) -&gt; None\n</code></pre> <p>Perform final plugin teardown.</p> <p>This function is called once after all test are executed and before test process is exited.</p> <p>See pytest.hookspec.pytest_unconfigure.</p> <p>Perform the following actions:</p> <ul> <li>Extract the plugin instance from the config object.</li> <li>Close the plugin instance.</li> <li>Delete the plugin instance from the config object.</li> </ul> Source code in <code>src/pytest_broadcaster/plugin.py</code> <pre><code>def pytest_unconfigure(config: pytest.Config) -&gt; None:\n    \"\"\"Perform final plugin teardown.\n\n    This function is called once after all test are executed and before test process is\n    exited.\n\n    See [pytest.hookspec.pytest_unconfigure][_pytest.hookspec.pytest_unconfigure].\n\n    Perform the following actions:\n\n    - Extract the plugin instance from the config object.\n    - Close the plugin instance.\n    - Delete the plugin instance from the config object.\n    \"\"\"\n    plugin: PytestBroadcasterPlugin | None = getattr(config, __PLUGIN_ATTR__, None)\n    if plugin:\n        plugin.close()\n        config.pluginmanager.unregister(plugin)\n        delattr(config, __PLUGIN_ATTR__)\n</code></pre>"},{"location":"reference/plugin/#pytest_broadcaster.plugin.PytestBroadcasterPlugin","title":"PytestBroadcasterPlugin","text":"<pre><code>PytestBroadcasterPlugin(\n    config: Config,\n    reporter: Reporter,\n    publishers: list[Destination],\n)\n</code></pre> <p>A pytest plugin to log collection to a line-based JSON file.</p> <p>Methods:</p> Name Description <code>close</code> <p>Close the plugin instance.</p> <code>open</code> <p>Open the plugin instance.</p> <code>pytest_collectreport</code> <p>Collector finished collecting a node.</p> <code>pytest_exception_interact</code> <p>Collector encountered an error.</p> <code>pytest_runtest_logfinish</code> <p>Pytest calls this function after running the runtest protocol for a single item.</p> <code>pytest_runtest_logreport</code> <p>Process the TestReport produced for each of the setup, call and teardown runtest steps of a test case.</p> <code>pytest_sessionfinish</code> <p>Write a session end event.</p> <code>pytest_sessionstart</code> <p>Write a session start event.</p> <code>pytest_terminal_summary</code> <p>Add a section to terminal summary reporting.</p> <code>pytest_warning_recorded</code> <p>Process a warning captured during the session.</p> Source code in <code>src/pytest_broadcaster/plugin.py</code> <pre><code>def __init__(\n    self,\n    config: pytest.Config,\n    reporter: Reporter,\n    publishers: list[Destination],\n) -&gt; None:\n    \"\"\"Create a new pytest broadcaster plugin.\"\"\"\n    self.config = config\n    self.publishers = publishers\n    self.reporter = reporter\n    self.stack = ExitStack()\n</code></pre>"},{"location":"reference/plugin/#pytest_broadcaster.plugin.PytestBroadcasterPlugin.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the plugin instance.</p> <p>Perform the following actions:</p> <ul> <li>Close the JSON Lines output file (if any).</li> <li>Write the results to the JSON output file (if any)</li> </ul> Source code in <code>src/pytest_broadcaster/plugin.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the plugin instance.\n\n    Perform the following actions:\n\n    - Close the JSON Lines output file (if any).\n    - Write the results to the JSON output file (if any)\n    \"\"\"\n    if result := self.reporter.make_session_result():\n        self._write_result(result)\n    self.stack.close()\n</code></pre>"},{"location":"reference/plugin/#pytest_broadcaster.plugin.PytestBroadcasterPlugin.open","title":"open","text":"<pre><code>open() -&gt; None\n</code></pre> <p>Open the plugin instance.</p> <p>Perform the following actions:</p> <ul> <li>Skip if there is no JSON Lines output</li> <li>Raise an error if the JSON Lines output file is already open.</li> <li>Ensure the parent directory of JSON Lines output file exists.</li> <li>Open the JSON Lines output file in write mode (erasing any previous content)</li> </ul> Source code in <code>src/pytest_broadcaster/plugin.py</code> <pre><code>def open(self) -&gt; None:\n    \"\"\"Open the plugin instance.\n\n    Perform the following actions:\n\n    - Skip if there is no JSON Lines output\n    - Raise an error if the JSON Lines output file is already open.\n    - Ensure the parent directory of JSON Lines output file exists.\n    - Open the JSON Lines output file in write mode (erasing any previous content)\n    \"\"\"\n    for publisher in self.publishers:\n        try:\n            self.stack.enter_context(publisher)\n        except Exception as e:  # noqa: PERF203, BLE001\n            warnings.warn(\n                f\"Failed to open publisher: {publisher} - {e!r}\", stacklevel=1\n            )\n</code></pre>"},{"location":"reference/plugin/#pytest_broadcaster.plugin.PytestBroadcasterPlugin.pytest_collectreport","title":"pytest_collectreport","text":"<pre><code>pytest_collectreport(report: CollectReport) -&gt; None\n</code></pre> <p>Collector finished collecting a node.</p> <p>See pytest.hookspec.pytest_collectreport.</p> Source code in <code>src/pytest_broadcaster/plugin.py</code> <pre><code>def pytest_collectreport(self, report: pytest.CollectReport) -&gt; None:\n    \"\"\"Collector finished collecting a node.\n\n    See [pytest.hookspec.pytest_collectreport][_pytest.hookspec.pytest_collectreport].\n    \"\"\"\n    # Skip if the report failed.\n    if report.failed:\n        return\n    self._write_event(self.reporter.make_collect_report(report))\n</code></pre>"},{"location":"reference/plugin/#pytest_broadcaster.plugin.PytestBroadcasterPlugin.pytest_exception_interact","title":"pytest_exception_interact","text":"<pre><code>pytest_exception_interact(\n    node: Item | Collector,\n    call: CallInfo[Any],\n    report: TestReport | CollectReport,\n) -&gt; None\n</code></pre> <p>Collector encountered an error.</p> <p>See pytest.hookspec.pytest_exception_interact.</p> Source code in <code>src/pytest_broadcaster/plugin.py</code> <pre><code>def pytest_exception_interact(\n    self,\n    node: pytest.Item | pytest.Collector,\n    call: pytest.CallInfo[Any],\n    report: pytest.TestReport | pytest.CollectReport,\n) -&gt; None:\n    \"\"\"Collector encountered an error.\n\n    See [pytest.hookspec.pytest_exception_interact][_pytest.hookspec.pytest_exception_interact].\n    \"\"\"\n    # Skip if the report is not a test report.\n    if isinstance(report, pytest.TestReport):\n        return\n    self._write_event(self.reporter.make_error_message(report, call))\n</code></pre>"},{"location":"reference/plugin/#pytest_broadcaster.plugin.PytestBroadcasterPlugin.pytest_runtest_logfinish","title":"pytest_runtest_logfinish","text":"<pre><code>pytest_runtest_logfinish(\n    nodeid: str, location: tuple[str, int | None, str]\n) -&gt; None\n</code></pre> <p>Pytest calls this function after running the runtest protocol for a single item.</p> <p>See pytest.hookspec.pytest_runtest_logfinish.</p> Source code in <code>src/pytest_broadcaster/plugin.py</code> <pre><code>def pytest_runtest_logfinish(\n    self, nodeid: str, location: tuple[str, int | None, str]\n) -&gt; None:\n    \"\"\"Pytest calls this function after running the runtest protocol for a single item.\n\n    See [pytest.hookspec.pytest_runtest_logfinish][_pytest.hookspec.pytest_runtest_logfinish].\n    \"\"\"\n    self._write_event(self.reporter.make_test_case_end(nodeid))\n</code></pre>"},{"location":"reference/plugin/#pytest_broadcaster.plugin.PytestBroadcasterPlugin.pytest_runtest_logreport","title":"pytest_runtest_logreport","text":"<pre><code>pytest_runtest_logreport(report: TestReport) -&gt; None\n</code></pre> <p>Process the TestReport produced for each of the setup, call and teardown runtest steps of a test case.</p> <p>See pytest.hookspec.pytest_runtest_logreport.</p> Source code in <code>src/pytest_broadcaster/plugin.py</code> <pre><code>def pytest_runtest_logreport(self, report: pytest.TestReport) -&gt; None:\n    \"\"\"Process the [TestReport][pytest.TestReport] produced for each of the setup, call and teardown runtest steps of a test case.\n\n    See [pytest.hookspec.pytest_runtest_logreport][_pytest.hookspec.pytest_runtest_logreport].\n    \"\"\"\n    self._write_event(self.reporter.make_test_case_step(report))\n</code></pre>"},{"location":"reference/plugin/#pytest_broadcaster.plugin.PytestBroadcasterPlugin.pytest_sessionfinish","title":"pytest_sessionfinish","text":"<pre><code>pytest_sessionfinish(exitstatus: int) -&gt; None\n</code></pre> <p>Write a session end event.</p> <p>This function is called after whole test run finished, right before returning the exit status to the system.</p> <p>See pytest.hookspec.pytest_sessionfinish</p> Source code in <code>src/pytest_broadcaster/plugin.py</code> <pre><code>def pytest_sessionfinish(self, exitstatus: int) -&gt; None:\n    \"\"\"Write a session end event.\n\n    This function is called after whole test run finished, right before returning\n    the exit status to the system.\n\n    See [pytest.hookspec.pytest_sessionfinish][_pytest.hookspec.pytest_sessionfinish]\n    \"\"\"\n    self._write_event(self.reporter.make_session_end(exitstatus))\n</code></pre>"},{"location":"reference/plugin/#pytest_broadcaster.plugin.PytestBroadcasterPlugin.pytest_sessionstart","title":"pytest_sessionstart","text":"<pre><code>pytest_sessionstart() -&gt; None\n</code></pre> <p>Write a session start event.</p> <p>This function is called after the Session object has been created and before performing collection and entering the run test loop.</p> <p>See pytest.hookspec.pytest_sessionstart.</p> Source code in <code>src/pytest_broadcaster/plugin.py</code> <pre><code>def pytest_sessionstart(self) -&gt; None:\n    \"\"\"Write a session start event.\n\n    This function is called after the [Session object][pytest.Session] has been\n    created and before performing collection and entering the run test loop.\n\n    See [pytest.hookspec.pytest_sessionstart][_pytest.hookspec.pytest_sessionstart].\n    \"\"\"\n    self._write_event(self.reporter.make_session_start())\n</code></pre>"},{"location":"reference/plugin/#pytest_broadcaster.plugin.PytestBroadcasterPlugin.pytest_terminal_summary","title":"pytest_terminal_summary","text":"<pre><code>pytest_terminal_summary(\n    terminalreporter: TerminalReporter,\n) -&gt; None\n</code></pre> <p>Add a section to terminal summary reporting.</p> <p>See pytest.hookspec.pytest_terminal_summary.</p> Source code in <code>src/pytest_broadcaster/plugin.py</code> <pre><code>def pytest_terminal_summary(self, terminalreporter: TerminalReporter) -&gt; None:\n    \"\"\"Add a section to terminal summary reporting.\n\n    See [pytest.hookspec.pytest_terminal_summary][_pytest.hookspec.pytest_terminal_summary].\n    \"\"\"\n    for publisher in self.publishers:\n        if summary := publisher.summary():\n            terminalreporter.write_sep(\"-\", f\"generated report log file: {summary}\")\n</code></pre>"},{"location":"reference/plugin/#pytest_broadcaster.plugin.PytestBroadcasterPlugin.pytest_warning_recorded","title":"pytest_warning_recorded","text":"<pre><code>pytest_warning_recorded(\n    warning_message: WarningMessage,\n    when: Literal[\"config\", \"collect\", \"runtest\"],\n    nodeid: str,\n    location: tuple[str, int, str] | None,\n) -&gt; None\n</code></pre> <p>Process a warning captured during the session.</p> <p>See pytest.hookspec.pytest_warning_recorded.</p> Source code in <code>src/pytest_broadcaster/plugin.py</code> <pre><code>def pytest_warning_recorded(\n    self,\n    warning_message: warnings.WarningMessage,\n    when: Literal[\"config\", \"collect\", \"runtest\"],\n    nodeid: str,\n    location: tuple[str, int, str] | None,\n) -&gt; None:\n    \"\"\"Process a warning captured during the session.\n\n    See [pytest.hookspec.pytest_warning_recorded][_pytest.hookspec.pytest_warning_recorded].\n    \"\"\"\n    self._write_event(\n        self.reporter.make_warning_message(\n            warning_message=warning_message,\n            when=when,\n            nodeid=nodeid,\n        )\n    )\n</code></pre>"},{"location":"reference/events/","title":"Session Event","text":"<p>JSON Schema</p> <p>https://charbonnierg.github.io/pytest-broadcaster/latest/schemas/session_event.json</p> <p><code>SessionEvent</code> is a type alias for the union of all possible events that can be emitted during a test session.</p>"},{"location":"reference/events/#pytest_broadcaster.models.session_event.SessionEvent","title":"SessionEvent  <code>module-attribute</code>","text":"<pre><code>SessionEvent = Union[\n    CollectReport,\n    TestCaseEnd,\n    TestCaseSetup,\n    TestCaseTeardown,\n    TestCaseCall,\n    ErrorMessage,\n    WarningMessage,\n    SessionStart,\n    SessionEnd,\n]\n</code></pre>"},{"location":"reference/events/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Session Events</li> <li>Session Start</li> <li>Collect Report<ul> <li>Test Directory</li> <li>Test Module</li> <li>Test Suite</li> <li>Test Case</li> </ul> </li> <li>Test Case Steps<ul> <li>Test Case Setup</li> <li>Test Case Call</li> <li>Test Case Teardown</li> <li>Test Case End</li> <li>Test Case Fields<ul> <li>Test Case Error</li> <li>Outcome</li> </ul> </li> </ul> </li> <li>Warning Message</li> <li>Error Message</li> <li>Warning &amp; Errors Fields<ul> <li>Location</li> <li>Traceback</li> </ul> </li> <li>Session End</li> </ul>"},{"location":"reference/events/error_message/","title":"Error Message","text":"<p>JSON Schema</p> <p>https://charbonnierg.github.io/pytest-broadcaster/latest/schemas/error_message.json</p>"},{"location":"reference/events/error_message/#pytest_broadcaster.models.error_message.ErrorMessage","title":"ErrorMessage  <code>dataclass</code>","text":"<pre><code>ErrorMessage(\n    when: When,\n    location: Location,\n    exception_type: str,\n    exception_value: str,\n    traceback: Traceback,\n    event: str = \"error_message\",\n)\n</code></pre> <p>An error message.</p> <p>Attributes:</p> Name Type Description <code>event</code> <code>str</code> <p>The event type. Always 'error_message'.</p> <code>exception_type</code> <code>str</code> <p>The exception type as a string.</p> <code>exception_value</code> <code>str</code> <p>The exception value as a string.</p> <code>location</code> <code>Location</code> <p>The location of the error.</p> <code>traceback</code> <code>Traceback</code> <p>The traceback of the error. A traceback contains entries for each frame of the call stack.</p> <code>when</code> <code>When</code> <p>When the error message is emitted.</p>"},{"location":"reference/events/error_message/#pytest_broadcaster.models.error_message.ErrorMessage.event","title":"event  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>event: str = 'error_message'\n</code></pre> <p>The event type. Always 'error_message'.</p>"},{"location":"reference/events/error_message/#pytest_broadcaster.models.error_message.ErrorMessage.exception_type","title":"exception_type  <code>instance-attribute</code>","text":"<pre><code>exception_type: str\n</code></pre> <p>The exception type as a string.</p>"},{"location":"reference/events/error_message/#pytest_broadcaster.models.error_message.ErrorMessage.exception_value","title":"exception_value  <code>instance-attribute</code>","text":"<pre><code>exception_value: str\n</code></pre> <p>The exception value as a string.</p>"},{"location":"reference/events/error_message/#pytest_broadcaster.models.error_message.ErrorMessage.location","title":"location  <code>instance-attribute</code>","text":"<pre><code>location: Location\n</code></pre> <p>The location of the error.</p>"},{"location":"reference/events/error_message/#pytest_broadcaster.models.error_message.ErrorMessage.traceback","title":"traceback  <code>instance-attribute</code>","text":"<pre><code>traceback: Traceback\n</code></pre> <p>The traceback of the error. A traceback contains entries for each frame of the call stack.</p>"},{"location":"reference/events/error_message/#pytest_broadcaster.models.error_message.ErrorMessage.when","title":"when  <code>instance-attribute</code>","text":"<pre><code>when: When\n</code></pre> <p>When the error message is emitted.</p>"},{"location":"reference/events/location/","title":"Location","text":"<p>JSON Schema</p> <p>https://charbonnierg.github.io/pytest-broadcaster/latest/schemas/location.json</p>"},{"location":"reference/events/location/#pytest_broadcaster.models.location.Location","title":"Location  <code>dataclass</code>","text":"<pre><code>Location(filename: str, lineno: int)\n</code></pre> <p>Location in code source.</p> <p>Attributes:</p> Name Type Description <code>filename</code> <code>str</code> <p>File name.</p> <code>lineno</code> <code>int</code> <p>Line number.</p>"},{"location":"reference/events/location/#pytest_broadcaster.models.location.Location.filename","title":"filename  <code>instance-attribute</code>","text":"<pre><code>filename: str\n</code></pre> <p>File name.</p>"},{"location":"reference/events/location/#pytest_broadcaster.models.location.Location.lineno","title":"lineno  <code>instance-attribute</code>","text":"<pre><code>lineno: int\n</code></pre> <p>Line number.</p>"},{"location":"reference/events/project/","title":"Project","text":"<p>JSON Schema</p> <p>https://charbonnierg.github.io/pytest-broadcaster/latest/schemas/project.json</p>"},{"location":"reference/events/project/#pytest_broadcaster.models.project.Project","title":"Project  <code>dataclass</code>","text":"<pre><code>Project(\n    name: str,\n    version: str | None = None,\n    repository_url: str | None = None,\n)\n</code></pre> <p>Metadata about the project for which tests are run.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the project. Project name is detected from pyproject.toml file if the file exits.</p> <code>repository_url</code> <code>str | None</code> <p>The URL of the project's repository.  Repositor URL is detected from pyproject.toml file if the file exits. The fields 'repository' and 'source' are looked for (case-insensitive) in respective order, and the first one found is used.</p> <code>version</code> <code>str | None</code> <p>The version of the project. Project version is detected using importlib.metadata if pyproject.toml file if the file exits.</p>"},{"location":"reference/events/project/#pytest_broadcaster.models.project.Project.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the project. Project name is detected from pyproject.toml file if the file exits.</p>"},{"location":"reference/events/project/#pytest_broadcaster.models.project.Project.repository_url","title":"repository_url  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>repository_url: str | None = None\n</code></pre> <p>The URL of the project's repository.  Repositor URL is detected from pyproject.toml file if the file exits. The fields 'repository' and 'source' are looked for (case-insensitive) in respective order, and the first one found is used.</p>"},{"location":"reference/events/project/#pytest_broadcaster.models.project.Project.version","title":"version  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>version: str | None = None\n</code></pre> <p>The version of the project. Project version is detected using importlib.metadata if pyproject.toml file if the file exits.</p>"},{"location":"reference/events/python/","title":"Python","text":"<p>JSON Schema</p> <p>https://charbonnierg.github.io/pytest-broadcaster/latest/schemas/python_distribution.json</p>"},{"location":"reference/events/python/#pytest_broadcaster.models.python_distribution.PythonDistribution","title":"PythonDistribution  <code>dataclass</code>","text":"<pre><code>PythonDistribution(\n    version: Version,\n    processor: str,\n    platform: Platform,\n    packages: list[Package],\n)\n</code></pre> <p>Metadata about the python interpreter being used to run the tests.</p> <p>Attributes:</p> Name Type Description <code>packages</code> <code>list[Package]</code> <p>The packages installed in the python interpreter.</p> <code>platform</code> <code>Platform</code> <p>The platform of the python interpreter.</p> <code>processor</code> <code>str</code> <p>The processor architecture of the python interpreter.</p> <code>version</code> <code>Version</code> <p>The version of the python interpreter being used to run the tests.</p>"},{"location":"reference/events/python/#pytest_broadcaster.models.python_distribution.PythonDistribution.packages","title":"packages  <code>instance-attribute</code>","text":"<pre><code>packages: list[Package]\n</code></pre> <p>The packages installed in the python interpreter.</p>"},{"location":"reference/events/python/#pytest_broadcaster.models.python_distribution.PythonDistribution.platform","title":"platform  <code>instance-attribute</code>","text":"<pre><code>platform: Platform\n</code></pre> <p>The platform of the python interpreter.</p>"},{"location":"reference/events/python/#pytest_broadcaster.models.python_distribution.PythonDistribution.processor","title":"processor  <code>instance-attribute</code>","text":"<pre><code>processor: str\n</code></pre> <p>The processor architecture of the python interpreter.</p>"},{"location":"reference/events/python/#pytest_broadcaster.models.python_distribution.PythonDistribution.version","title":"version  <code>instance-attribute</code>","text":"<pre><code>version: Version\n</code></pre> <p>The version of the python interpreter being used to run the tests.</p>"},{"location":"reference/events/python/#pytest_broadcaster.models.python_distribution.Version","title":"Version  <code>dataclass</code>","text":"<pre><code>Version(\n    major: int,\n    minor: int,\n    micro: int,\n    releaselevel: Releaselevel,\n)\n</code></pre> <p>The version of the python interpreter being used to run the tests.</p> <p>Attributes:</p> Name Type Description <code>major</code> <code>int</code> <p>The major version of the python interpreter.</p> <code>micro</code> <code>int</code> <p>The micro version of the python interpreter.</p> <code>minor</code> <code>int</code> <p>The minor version of the python interpreter.</p> <code>releaselevel</code> <code>Releaselevel</code> <p>The release level of the python interpreter.</p>"},{"location":"reference/events/python/#pytest_broadcaster.models.python_distribution.Version.major","title":"major  <code>instance-attribute</code>","text":"<pre><code>major: int\n</code></pre> <p>The major version of the python interpreter.</p>"},{"location":"reference/events/python/#pytest_broadcaster.models.python_distribution.Version.micro","title":"micro  <code>instance-attribute</code>","text":"<pre><code>micro: int\n</code></pre> <p>The micro version of the python interpreter.</p>"},{"location":"reference/events/python/#pytest_broadcaster.models.python_distribution.Version.minor","title":"minor  <code>instance-attribute</code>","text":"<pre><code>minor: int\n</code></pre> <p>The minor version of the python interpreter.</p>"},{"location":"reference/events/python/#pytest_broadcaster.models.python_distribution.Version.releaselevel","title":"releaselevel  <code>instance-attribute</code>","text":"<pre><code>releaselevel: Releaselevel\n</code></pre> <p>The release level of the python interpreter.</p>"},{"location":"reference/events/python/#pytest_broadcaster.models.python_distribution.Releaselevel","title":"Releaselevel","text":"<p>               Bases: <code>Enum</code></p> <p>The release level of the python interpreter.</p> <p>Attributes:</p> Name Type Description <code>alpha</code> <code>beta</code> <code>candidate</code> <code>final</code>"},{"location":"reference/events/python/#pytest_broadcaster.models.python_distribution.Releaselevel.alpha","title":"alpha  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>alpha = 'alpha'\n</code></pre>"},{"location":"reference/events/python/#pytest_broadcaster.models.python_distribution.Releaselevel.beta","title":"beta  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>beta = 'beta'\n</code></pre>"},{"location":"reference/events/python/#pytest_broadcaster.models.python_distribution.Releaselevel.candidate","title":"candidate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>candidate = 'candidate'\n</code></pre>"},{"location":"reference/events/python/#pytest_broadcaster.models.python_distribution.Releaselevel.final","title":"final  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>final = 'final'\n</code></pre>"},{"location":"reference/events/python/#pytest_broadcaster.models.python_distribution.Platform","title":"Platform","text":"<p>               Bases: <code>Enum</code></p> <p>The platform of the python interpreter.</p> <p>Attributes:</p> Name Type Description <code>darwin</code> <code>java</code> <code>linux</code> <code>unknown</code> <code>windows</code>"},{"location":"reference/events/python/#pytest_broadcaster.models.python_distribution.Platform.darwin","title":"darwin  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>darwin = 'darwin'\n</code></pre>"},{"location":"reference/events/python/#pytest_broadcaster.models.python_distribution.Platform.java","title":"java  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>java = 'java'\n</code></pre>"},{"location":"reference/events/python/#pytest_broadcaster.models.python_distribution.Platform.linux","title":"linux  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>linux = 'linux'\n</code></pre>"},{"location":"reference/events/python/#pytest_broadcaster.models.python_distribution.Platform.unknown","title":"unknown  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>unknown = 'unknown'\n</code></pre>"},{"location":"reference/events/python/#pytest_broadcaster.models.python_distribution.Platform.windows","title":"windows  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>windows = 'windows'\n</code></pre>"},{"location":"reference/events/session_end/","title":"Session End","text":"<p>JSON Schema</p> <p>https://charbonnierg.github.io/pytest-broadcaster/latest/schemas/session_end.json</p>"},{"location":"reference/events/session_end/#pytest_broadcaster.models.session_end.SessionEnd","title":"SessionEnd  <code>dataclass</code>","text":"<pre><code>SessionEnd(\n    session_id: str,\n    timestamp: str,\n    exit_status: int,\n    event: str = \"session_end\",\n)\n</code></pre> <p>Event emitted when the session is finished.</p> <p>Attributes:</p> Name Type Description <code>event</code> <code>str</code> <p>The event type. Always set to 'session_end'.</p> <code>exit_status</code> <code>int</code> <p>The status which pytest will return to the system. 0 means success, 1 means test failure, anything else is an error.</p> <code>session_id</code> <code>str</code> <p>The unique if of this test session used to aggregate events together.</p> <code>timestamp</code> <code>str</code> <p>The time when the session finished in ISO 8601 format.</p>"},{"location":"reference/events/session_end/#pytest_broadcaster.models.session_end.SessionEnd.event","title":"event  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>event: str = 'session_end'\n</code></pre> <p>The event type. Always set to 'session_end'.</p>"},{"location":"reference/events/session_end/#pytest_broadcaster.models.session_end.SessionEnd.exit_status","title":"exit_status  <code>instance-attribute</code>","text":"<pre><code>exit_status: int\n</code></pre> <p>The status which pytest will return to the system. 0 means success, 1 means test failure, anything else is an error.</p>"},{"location":"reference/events/session_end/#pytest_broadcaster.models.session_end.SessionEnd.session_id","title":"session_id  <code>instance-attribute</code>","text":"<pre><code>session_id: str\n</code></pre> <p>The unique if of this test session used to aggregate events together.</p>"},{"location":"reference/events/session_end/#pytest_broadcaster.models.session_end.SessionEnd.timestamp","title":"timestamp  <code>instance-attribute</code>","text":"<pre><code>timestamp: str\n</code></pre> <p>The time when the session finished in ISO 8601 format.</p>"},{"location":"reference/events/session_start/","title":"Session Start Event","text":"<p>JSON Schema</p> <p>https://charbonnierg.github.io/pytest-broadcaster/latest/schemas/session_start.json</p>"},{"location":"reference/events/session_start/#pytest_broadcaster.models.session_start.SessionStart","title":"SessionStart  <code>dataclass</code>","text":"<pre><code>SessionStart(\n    session_id: str,\n    timestamp: str,\n    python: PythonDistribution,\n    pytest_version: str,\n    plugin_version: str,\n    event: str = \"session_start\",\n    project: Project | None = None,\n)\n</code></pre> <p>Event emitted when the test session is started.</p> <p>Attributes:</p> Name Type Description <code>event</code> <code>str</code> <p>The event type. Always set to <code>session_start</code>.</p> <code>plugin_version</code> <code>str</code> <p>The version of pytest-broadcaster plugin that is used to produce the report.</p> <code>project</code> <code>Project | None</code> <p>The project that is being tested.</p> <code>pytest_version</code> <code>str</code> <p>The version of pytest that is running the tests.</p> <code>python</code> <code>PythonDistribution</code> <p>The Python distribution that is running the tests.</p> <code>session_id</code> <code>str</code> <p>The unique if of this test session used to aggregate events together.</p> <code>timestamp</code> <code>str</code> <p>The date and time when the test session was started in ISO 8601 format.</p>"},{"location":"reference/events/session_start/#pytest_broadcaster.models.session_start.SessionStart.event","title":"event  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>event: str = 'session_start'\n</code></pre> <p>The event type. Always set to <code>session_start</code>.</p>"},{"location":"reference/events/session_start/#pytest_broadcaster.models.session_start.SessionStart.plugin_version","title":"plugin_version  <code>instance-attribute</code>","text":"<pre><code>plugin_version: str\n</code></pre> <p>The version of pytest-broadcaster plugin that is used to produce the report.</p>"},{"location":"reference/events/session_start/#pytest_broadcaster.models.session_start.SessionStart.project","title":"project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>project: Project | None = None\n</code></pre> <p>The project that is being tested.</p>"},{"location":"reference/events/session_start/#pytest_broadcaster.models.session_start.SessionStart.pytest_version","title":"pytest_version  <code>instance-attribute</code>","text":"<pre><code>pytest_version: str\n</code></pre> <p>The version of pytest that is running the tests.</p>"},{"location":"reference/events/session_start/#pytest_broadcaster.models.session_start.SessionStart.python","title":"python  <code>instance-attribute</code>","text":"<pre><code>python: PythonDistribution\n</code></pre> <p>The Python distribution that is running the tests.</p>"},{"location":"reference/events/session_start/#pytest_broadcaster.models.session_start.SessionStart.session_id","title":"session_id  <code>instance-attribute</code>","text":"<pre><code>session_id: str\n</code></pre> <p>The unique if of this test session used to aggregate events together.</p>"},{"location":"reference/events/session_start/#pytest_broadcaster.models.session_start.SessionStart.timestamp","title":"timestamp  <code>instance-attribute</code>","text":"<pre><code>timestamp: str\n</code></pre> <p>The date and time when the test session was started in ISO 8601 format.</p>"},{"location":"reference/events/traceback/","title":"Traceback","text":"<p>JSON Schema</p> <p>https://charbonnierg.github.io/pytest-broadcaster/latest/schemas/traceback.json</p>"},{"location":"reference/events/traceback/#pytest_broadcaster.models.traceback.Traceback","title":"Traceback  <code>dataclass</code>","text":"<pre><code>Traceback(entries: list[Entry])\n</code></pre> <p>An error traceback.</p> <p>Attributes:</p> Name Type Description <code>entries</code> <code>list[Entry]</code> <p>Traceback frame entries.</p>"},{"location":"reference/events/traceback/#pytest_broadcaster.models.traceback.Traceback.entries","title":"entries  <code>instance-attribute</code>","text":"<pre><code>entries: list[Entry]\n</code></pre> <p>Traceback frame entries.</p>"},{"location":"reference/events/warning_message/","title":"Warning Message","text":"<p>JSON Schema</p> <p>https://charbonnierg.github.io/pytest-broadcaster/latest/schemas/warning_message.json</p>"},{"location":"reference/events/warning_message/#pytest_broadcaster.models.warning_message.WarningMessage","title":"WarningMessage  <code>dataclass</code>","text":"<pre><code>WarningMessage(\n    when: When,\n    node_id: str,\n    location: Location,\n    message: str,\n    event: str = \"warning_message\",\n    category: str | None = None,\n)\n</code></pre> <p>A warning message.</p> <p>Attributes:</p> Name Type Description <code>category</code> <code>str | None</code> <p>The category of the warning message.</p> <code>event</code> <code>str</code> <p>The event type. Always 'warning_message'.</p> <code>location</code> <code>Location</code> <p>The location of the warning message.</p> <code>message</code> <code>str</code> <p>The string content of the warning message.</p> <code>node_id</code> <code>str</code> <p>The node ID of the node where the warning message is emitted.</p> <code>when</code> <code>When</code> <p>When the warning message is emitted.</p>"},{"location":"reference/events/warning_message/#pytest_broadcaster.models.warning_message.WarningMessage.category","title":"category  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>category: str | None = None\n</code></pre> <p>The category of the warning message.</p>"},{"location":"reference/events/warning_message/#pytest_broadcaster.models.warning_message.WarningMessage.event","title":"event  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>event: str = 'warning_message'\n</code></pre> <p>The event type. Always 'warning_message'.</p>"},{"location":"reference/events/warning_message/#pytest_broadcaster.models.warning_message.WarningMessage.location","title":"location  <code>instance-attribute</code>","text":"<pre><code>location: Location\n</code></pre> <p>The location of the warning message.</p>"},{"location":"reference/events/warning_message/#pytest_broadcaster.models.warning_message.WarningMessage.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message: str\n</code></pre> <p>The string content of the warning message.</p>"},{"location":"reference/events/warning_message/#pytest_broadcaster.models.warning_message.WarningMessage.node_id","title":"node_id  <code>instance-attribute</code>","text":"<pre><code>node_id: str\n</code></pre> <p>The node ID of the node where the warning message is emitted.</p>"},{"location":"reference/events/warning_message/#pytest_broadcaster.models.warning_message.WarningMessage.when","title":"when  <code>instance-attribute</code>","text":"<pre><code>when: When\n</code></pre> <p>When the warning message is emitted.</p>"},{"location":"reference/events/collect_report/","title":"Collect Report","text":"<p>JSON Schema</p> <p>https://charbonnierg.github.io/pytest-broadcaster/latest/schemas/collect_report.json</p>"},{"location":"reference/events/collect_report/#pytest_broadcaster.models.collect_report.CollectReport","title":"CollectReport  <code>dataclass</code>","text":"<pre><code>CollectReport(\n    session_id: str,\n    node_id: str,\n    timestamp: str,\n    items: list[\n        TestDirectory | TestModule | TestSuite | TestCase\n    ],\n    event: str = \"collect_report\",\n)\n</code></pre> <p>A report of collected items.</p> <p>Attributes:</p> Name Type Description <code>event</code> <code>str</code> <p>The event type. Always set to 'collect_report'.</p> <code>items</code> <code>list[TestDirectory | TestModule | TestSuite | TestCase]</code> <p>An array of collected items. Each collected item is a test directory, test module, test suite, or test case. Top level directory is always first element in the array, followed by test cases, then test suites, then test modules, then test directories for each level of nesting.</p> <code>node_id</code> <code>str</code> <p>The node id of the node for which items were collected (the top level root directory has an empty node id).</p> <code>session_id</code> <code>str</code> <p>The unique if of this test session used to aggregate events together.</p> <code>timestamp</code> <code>str</code> <p>The date and time when the report was generated in ISO 8601 format.</p>"},{"location":"reference/events/collect_report/#pytest_broadcaster.models.collect_report.CollectReport.event","title":"event  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>event: str = 'collect_report'\n</code></pre> <p>The event type. Always set to 'collect_report'.</p>"},{"location":"reference/events/collect_report/#pytest_broadcaster.models.collect_report.CollectReport.items","title":"items  <code>instance-attribute</code>","text":"<pre><code>items: list[\n    TestDirectory | TestModule | TestSuite | TestCase\n]\n</code></pre> <p>An array of collected items. Each collected item is a test directory, test module, test suite, or test case. Top level directory is always first element in the array, followed by test cases, then test suites, then test modules, then test directories for each level of nesting.</p>"},{"location":"reference/events/collect_report/#pytest_broadcaster.models.collect_report.CollectReport.node_id","title":"node_id  <code>instance-attribute</code>","text":"<pre><code>node_id: str\n</code></pre> <p>The node id of the node for which items were collected (the top level root directory has an empty node id).</p>"},{"location":"reference/events/collect_report/#pytest_broadcaster.models.collect_report.CollectReport.session_id","title":"session_id  <code>instance-attribute</code>","text":"<pre><code>session_id: str\n</code></pre> <p>The unique if of this test session used to aggregate events together.</p>"},{"location":"reference/events/collect_report/#pytest_broadcaster.models.collect_report.CollectReport.timestamp","title":"timestamp  <code>instance-attribute</code>","text":"<pre><code>timestamp: str\n</code></pre> <p>The date and time when the report was generated in ISO 8601 format.</p>"},{"location":"reference/events/collect_report/test_case/","title":"Test Case","text":"<p>JSON Schema</p> <p>https://charbonnierg.github.io/pytest-broadcaster/latest/schemas/test_case.json</p>"},{"location":"reference/events/collect_report/test_case/#pytest_broadcaster.models.test_case.TestCase","title":"TestCase  <code>dataclass</code>","text":"<pre><code>TestCase(\n    node_id: str,\n    path: str,\n    name: str,\n    doc: str,\n    markers: list[str],\n    parameters: dict[str, str],\n    node_type: str = \"case\",\n    module: str | None = None,\n    suite: str | None = None,\n    function: str | None = None,\n)\n</code></pre> <p>A collected test case.</p> <p>Attributes:</p> Name Type Description <code>doc</code> <code>str</code> <p>The test docstring (optional).</p> <code>function</code> <code>str | None</code> <p>The function name (optional).</p> <code>markers</code> <code>list[str]</code> <p>The test markers. Each marker is a string.</p> <code>module</code> <code>str | None</code> <p>The module name where the test case is defined (optional).</p> <code>name</code> <code>str</code> <p>Test Name</p> <code>node_id</code> <code>str</code> <p>The node ID of the test case.</p> <code>node_type</code> <code>str</code> <p>The node type. Always set to 'case'.</p> <code>parameters</code> <code>dict[str, str]</code> <p>Test parameters names and types. Each key is a parameter name and each value is a parameter type as a string.</p> <code>path</code> <code>str</code> <p>The file path where the test case is defined.</p> <code>suite</code> <code>str | None</code> <p>The test suite name where the test case is defined (optional).</p>"},{"location":"reference/events/collect_report/test_case/#pytest_broadcaster.models.test_case.TestCase.doc","title":"doc  <code>instance-attribute</code>","text":"<pre><code>doc: str\n</code></pre> <p>The test docstring (optional).</p>"},{"location":"reference/events/collect_report/test_case/#pytest_broadcaster.models.test_case.TestCase.function","title":"function  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>function: str | None = None\n</code></pre> <p>The function name (optional).</p>"},{"location":"reference/events/collect_report/test_case/#pytest_broadcaster.models.test_case.TestCase.markers","title":"markers  <code>instance-attribute</code>","text":"<pre><code>markers: list[str]\n</code></pre> <p>The test markers. Each marker is a string.</p>"},{"location":"reference/events/collect_report/test_case/#pytest_broadcaster.models.test_case.TestCase.module","title":"module  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>module: str | None = None\n</code></pre> <p>The module name where the test case is defined (optional).</p>"},{"location":"reference/events/collect_report/test_case/#pytest_broadcaster.models.test_case.TestCase.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>Test Name</p>"},{"location":"reference/events/collect_report/test_case/#pytest_broadcaster.models.test_case.TestCase.node_id","title":"node_id  <code>instance-attribute</code>","text":"<pre><code>node_id: str\n</code></pre> <p>The node ID of the test case.</p>"},{"location":"reference/events/collect_report/test_case/#pytest_broadcaster.models.test_case.TestCase.node_type","title":"node_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>node_type: str = 'case'\n</code></pre> <p>The node type. Always set to 'case'.</p>"},{"location":"reference/events/collect_report/test_case/#pytest_broadcaster.models.test_case.TestCase.parameters","title":"parameters  <code>instance-attribute</code>","text":"<pre><code>parameters: dict[str, str]\n</code></pre> <p>Test parameters names and types. Each key is a parameter name and each value is a parameter type as a string.</p>"},{"location":"reference/events/collect_report/test_case/#pytest_broadcaster.models.test_case.TestCase.path","title":"path  <code>instance-attribute</code>","text":"<pre><code>path: str\n</code></pre> <p>The file path where the test case is defined.</p>"},{"location":"reference/events/collect_report/test_case/#pytest_broadcaster.models.test_case.TestCase.suite","title":"suite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>suite: str | None = None\n</code></pre> <p>The test suite name where the test case is defined (optional).</p>"},{"location":"reference/events/collect_report/test_directory/","title":"Test Directory","text":"<p>JSON Schema</p> <p>https://charbonnierg.github.io/pytest-broadcaster/latest/schemas/test_directory.json</p>"},{"location":"reference/events/collect_report/test_directory/#pytest_broadcaster.models.test_directory.TestDirectory","title":"TestDirectory  <code>dataclass</code>","text":"<pre><code>TestDirectory(\n    node_id: str,\n    name: str,\n    path: str,\n    node_type: str = \"directory\",\n)\n</code></pre> <p>A collected test directory.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The directory name.</p> <code>node_id</code> <code>str</code> <p>The node ID of the directory.</p> <code>node_type</code> <code>str</code> <p>The node type. Always set to 'directory'.</p> <code>path</code> <code>str</code> <p>The directory path.</p>"},{"location":"reference/events/collect_report/test_directory/#pytest_broadcaster.models.test_directory.TestDirectory.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The directory name.</p>"},{"location":"reference/events/collect_report/test_directory/#pytest_broadcaster.models.test_directory.TestDirectory.node_id","title":"node_id  <code>instance-attribute</code>","text":"<pre><code>node_id: str\n</code></pre> <p>The node ID of the directory.</p>"},{"location":"reference/events/collect_report/test_directory/#pytest_broadcaster.models.test_directory.TestDirectory.node_type","title":"node_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>node_type: str = 'directory'\n</code></pre> <p>The node type. Always set to 'directory'.</p>"},{"location":"reference/events/collect_report/test_directory/#pytest_broadcaster.models.test_directory.TestDirectory.path","title":"path  <code>instance-attribute</code>","text":"<pre><code>path: str\n</code></pre> <p>The directory path.</p>"},{"location":"reference/events/collect_report/test_module/","title":"Test Module","text":"<p>JSON Schema</p> <p>https://charbonnierg.github.io/pytest-broadcaster/latest/schemas/test_module.json</p>"},{"location":"reference/events/collect_report/test_module/#pytest_broadcaster.models.test_module.TestModule","title":"TestModule  <code>dataclass</code>","text":"<pre><code>TestModule(\n    node_id: str,\n    name: str,\n    path: str,\n    doc: str,\n    markers: list[str],\n    node_type: str = \"module\",\n)\n</code></pre> <p>A collected test module (test file).</p> <p>Attributes:</p> Name Type Description <code>doc</code> <code>str</code> <p>The module docstring.</p> <code>markers</code> <code>list[str]</code> <p>Test markers. Each marker is a string.</p> <code>name</code> <code>str</code> <p>The module name.</p> <code>node_id</code> <code>str</code> <p>The node ID of the test module.</p> <code>node_type</code> <code>str</code> <p>The node type. Always set to 'module'.</p> <code>path</code> <code>str</code> <p>The module file path.</p>"},{"location":"reference/events/collect_report/test_module/#pytest_broadcaster.models.test_module.TestModule.doc","title":"doc  <code>instance-attribute</code>","text":"<pre><code>doc: str\n</code></pre> <p>The module docstring.</p>"},{"location":"reference/events/collect_report/test_module/#pytest_broadcaster.models.test_module.TestModule.markers","title":"markers  <code>instance-attribute</code>","text":"<pre><code>markers: list[str]\n</code></pre> <p>Test markers. Each marker is a string.</p>"},{"location":"reference/events/collect_report/test_module/#pytest_broadcaster.models.test_module.TestModule.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The module name.</p>"},{"location":"reference/events/collect_report/test_module/#pytest_broadcaster.models.test_module.TestModule.node_id","title":"node_id  <code>instance-attribute</code>","text":"<pre><code>node_id: str\n</code></pre> <p>The node ID of the test module.</p>"},{"location":"reference/events/collect_report/test_module/#pytest_broadcaster.models.test_module.TestModule.node_type","title":"node_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>node_type: str = 'module'\n</code></pre> <p>The node type. Always set to 'module'.</p>"},{"location":"reference/events/collect_report/test_module/#pytest_broadcaster.models.test_module.TestModule.path","title":"path  <code>instance-attribute</code>","text":"<pre><code>path: str\n</code></pre> <p>The module file path.</p>"},{"location":"reference/events/collect_report/test_suite/","title":"Test Suite","text":"<p>JSON Schema</p> <p>https://charbonnierg.github.io/pytest-broadcaster/latest/schemas/test_suite.json</p>"},{"location":"reference/events/collect_report/test_suite/#pytest_broadcaster.models.test_suite.TestSuite","title":"TestSuite  <code>dataclass</code>","text":"<pre><code>TestSuite(\n    node_id: str,\n    name: str,\n    module: str,\n    path: str,\n    doc: str,\n    markers: list[str],\n    node_type: str = \"suite\",\n)\n</code></pre> <p>A collected test suite (test class).</p> <p>Attributes:</p> Name Type Description <code>doc</code> <code>str</code> <p>The suite docstring.</p> <code>markers</code> <code>list[str]</code> <p>Test markers. Each marker is a string.</p> <code>module</code> <code>str</code> <p>The name of the module where the suite is defined.</p> <code>name</code> <code>str</code> <p>The suite name, i.e., the name of the class.</p> <code>node_id</code> <code>str</code> <p>The node ID of the test suite.</p> <code>node_type</code> <code>str</code> <p>The node type. Always set to 'suite'.</p> <code>path</code> <code>str</code> <p>The name of the file where the suite is defined.</p>"},{"location":"reference/events/collect_report/test_suite/#pytest_broadcaster.models.test_suite.TestSuite.doc","title":"doc  <code>instance-attribute</code>","text":"<pre><code>doc: str\n</code></pre> <p>The suite docstring.</p>"},{"location":"reference/events/collect_report/test_suite/#pytest_broadcaster.models.test_suite.TestSuite.markers","title":"markers  <code>instance-attribute</code>","text":"<pre><code>markers: list[str]\n</code></pre> <p>Test markers. Each marker is a string.</p>"},{"location":"reference/events/collect_report/test_suite/#pytest_broadcaster.models.test_suite.TestSuite.module","title":"module  <code>instance-attribute</code>","text":"<pre><code>module: str\n</code></pre> <p>The name of the module where the suite is defined.</p>"},{"location":"reference/events/collect_report/test_suite/#pytest_broadcaster.models.test_suite.TestSuite.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The suite name, i.e., the name of the class.</p>"},{"location":"reference/events/collect_report/test_suite/#pytest_broadcaster.models.test_suite.TestSuite.node_id","title":"node_id  <code>instance-attribute</code>","text":"<pre><code>node_id: str\n</code></pre> <p>The node ID of the test suite.</p>"},{"location":"reference/events/collect_report/test_suite/#pytest_broadcaster.models.test_suite.TestSuite.node_type","title":"node_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>node_type: str = 'suite'\n</code></pre> <p>The node type. Always set to 'suite'.</p>"},{"location":"reference/events/collect_report/test_suite/#pytest_broadcaster.models.test_suite.TestSuite.path","title":"path  <code>instance-attribute</code>","text":"<pre><code>path: str\n</code></pre> <p>The name of the file where the suite is defined.</p>"},{"location":"reference/events/steps/","title":"Test Case Steps","text":"<p>For each test case, the following steps are executed:</p> <ul> <li>Test Case Setup</li> <li>Test Case Teardown</li> <li>Test Case End</li> </ul> <p>Optionally, the following steps can be executed before the teardown step (if test is not skipped or setup did not fail):</p> <ul> <li>Test Case Call</li> </ul> <p>The following sections describe the data structure of each step.</p>"},{"location":"reference/events/steps/outcome/","title":"Outcome","text":"<p>JSON Schema</p> <p>https://charbonnierg.github.io/pytest-broadcaster/latest/schemas/outcome.json</p>"},{"location":"reference/events/steps/outcome/#pytest_broadcaster.models.outcome.Outcome","title":"Outcome","text":"<p>               Bases: <code>Enum</code></p> <p>The outcome of a case step or a whole test case.</p>"},{"location":"reference/events/steps/test_case_call/","title":"Test Case Call","text":"<p>JSON Schema</p> <p>https://charbonnierg.github.io/pytest-broadcaster/latest/schemas/test_case_call.json</p>"},{"location":"reference/events/steps/test_case_call/#pytest_broadcaster.models.test_case_call.TestCaseCall","title":"TestCaseCall  <code>dataclass</code>","text":"<pre><code>TestCaseCall(\n    node_id: str,\n    session_id: str,\n    start_timestamp: str,\n    stop_timestamp: str,\n    duration: float,\n    outcome: Outcome,\n    event: str = \"case_call\",\n    error: TestCaseError | None = None,\n)\n</code></pre> <p>Event emitted when a call step has been executed in a test case.</p> <p>Attributes:</p> Name Type Description <code>duration</code> <code>float</code> <p>Duration of the call step in seconds.</p> <code>error</code> <code>TestCaseError | None</code> <p>Error details if the call step failed.</p> <code>event</code> <code>str</code> <p>The event type. Always set to 'case_call'.</p> <code>node_id</code> <code>str</code> <p>The node ID of the test case.</p> <code>outcome</code> <code>Outcome</code> <p>Outcome of the call step.</p> <code>session_id</code> <code>str</code> <p>The unique if of this test session used to aggregate events together.</p> <code>start_timestamp</code> <code>str</code> <p>Start time of the call step in ISO 8601 format.</p> <code>stop_timestamp</code> <code>str</code> <p>Stop time of the call step in ISO 8601 format.</p>"},{"location":"reference/events/steps/test_case_call/#pytest_broadcaster.models.test_case_call.TestCaseCall.duration","title":"duration  <code>instance-attribute</code>","text":"<pre><code>duration: float\n</code></pre> <p>Duration of the call step in seconds.</p>"},{"location":"reference/events/steps/test_case_call/#pytest_broadcaster.models.test_case_call.TestCaseCall.error","title":"error  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>error: TestCaseError | None = None\n</code></pre> <p>Error details if the call step failed.</p>"},{"location":"reference/events/steps/test_case_call/#pytest_broadcaster.models.test_case_call.TestCaseCall.event","title":"event  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>event: str = 'case_call'\n</code></pre> <p>The event type. Always set to 'case_call'.</p>"},{"location":"reference/events/steps/test_case_call/#pytest_broadcaster.models.test_case_call.TestCaseCall.node_id","title":"node_id  <code>instance-attribute</code>","text":"<pre><code>node_id: str\n</code></pre> <p>The node ID of the test case.</p>"},{"location":"reference/events/steps/test_case_call/#pytest_broadcaster.models.test_case_call.TestCaseCall.outcome","title":"outcome  <code>instance-attribute</code>","text":"<pre><code>outcome: Outcome\n</code></pre> <p>Outcome of the call step.</p>"},{"location":"reference/events/steps/test_case_call/#pytest_broadcaster.models.test_case_call.TestCaseCall.session_id","title":"session_id  <code>instance-attribute</code>","text":"<pre><code>session_id: str\n</code></pre> <p>The unique if of this test session used to aggregate events together.</p>"},{"location":"reference/events/steps/test_case_call/#pytest_broadcaster.models.test_case_call.TestCaseCall.start_timestamp","title":"start_timestamp  <code>instance-attribute</code>","text":"<pre><code>start_timestamp: str\n</code></pre> <p>Start time of the call step in ISO 8601 format.</p>"},{"location":"reference/events/steps/test_case_call/#pytest_broadcaster.models.test_case_call.TestCaseCall.stop_timestamp","title":"stop_timestamp  <code>instance-attribute</code>","text":"<pre><code>stop_timestamp: str\n</code></pre> <p>Stop time of the call step in ISO 8601 format.</p>"},{"location":"reference/events/steps/test_case_end/","title":"Test Case End","text":"<p>JSON Schema</p> <p>https://charbonnierg.github.io/pytest-broadcaster/latest/schemas/test_case_end.json</p>"},{"location":"reference/events/steps/test_case_end/#pytest_broadcaster.models.test_case_end.TestCaseEnd","title":"TestCaseEnd  <code>dataclass</code>","text":"<pre><code>TestCaseEnd(\n    session_id: str,\n    node_id: str,\n    start_timestamp: str,\n    stop_timestamp: str,\n    total_duration: float,\n    outcome: Outcome,\n    event: str = \"case_end\",\n)\n</code></pre> <p>Event emitted when a test case is finished (meaning all of setup, call and teardown steps are finished).</p> <p>Attributes:</p> Name Type Description <code>event</code> <code>str</code> <p>The event type. Always set to 'case_end'.</p> <code>node_id</code> <code>str</code> <p>The node ID of the test case.</p> <code>outcome</code> <code>Outcome</code> <p>Outcome of the test case.</p> <code>session_id</code> <code>str</code> <p>The unique if of this test session used to aggregate events together.</p> <code>start_timestamp</code> <code>str</code> <p>Start time of the test case (including setup).</p> <code>stop_timestamp</code> <code>str</code> <p>Stop time of the test case (including teardown).</p> <code>total_duration</code> <code>float</code> <p>Total duration of the test case in seconds (including setup and teardown).</p>"},{"location":"reference/events/steps/test_case_end/#pytest_broadcaster.models.test_case_end.TestCaseEnd.event","title":"event  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>event: str = 'case_end'\n</code></pre> <p>The event type. Always set to 'case_end'.</p>"},{"location":"reference/events/steps/test_case_end/#pytest_broadcaster.models.test_case_end.TestCaseEnd.node_id","title":"node_id  <code>instance-attribute</code>","text":"<pre><code>node_id: str\n</code></pre> <p>The node ID of the test case.</p>"},{"location":"reference/events/steps/test_case_end/#pytest_broadcaster.models.test_case_end.TestCaseEnd.outcome","title":"outcome  <code>instance-attribute</code>","text":"<pre><code>outcome: Outcome\n</code></pre> <p>Outcome of the test case.</p>"},{"location":"reference/events/steps/test_case_end/#pytest_broadcaster.models.test_case_end.TestCaseEnd.session_id","title":"session_id  <code>instance-attribute</code>","text":"<pre><code>session_id: str\n</code></pre> <p>The unique if of this test session used to aggregate events together.</p>"},{"location":"reference/events/steps/test_case_end/#pytest_broadcaster.models.test_case_end.TestCaseEnd.start_timestamp","title":"start_timestamp  <code>instance-attribute</code>","text":"<pre><code>start_timestamp: str\n</code></pre> <p>Start time of the test case (including setup).</p>"},{"location":"reference/events/steps/test_case_end/#pytest_broadcaster.models.test_case_end.TestCaseEnd.stop_timestamp","title":"stop_timestamp  <code>instance-attribute</code>","text":"<pre><code>stop_timestamp: str\n</code></pre> <p>Stop time of the test case (including teardown).</p>"},{"location":"reference/events/steps/test_case_end/#pytest_broadcaster.models.test_case_end.TestCaseEnd.total_duration","title":"total_duration  <code>instance-attribute</code>","text":"<pre><code>total_duration: float\n</code></pre> <p>Total duration of the test case in seconds (including setup and teardown).</p>"},{"location":"reference/events/steps/test_case_error/","title":"Test Case Error","text":"<p>JSON Schema</p> <p>https://charbonnierg.github.io/pytest-broadcaster/latest/schemas/test_case_error.json</p>"},{"location":"reference/events/steps/test_case_error/#pytest_broadcaster.models.test_case_error.TestCaseError","title":"TestCaseError  <code>dataclass</code>","text":"<pre><code>TestCaseError(\n    message: str, traceback: Traceback | None = None\n)\n</code></pre> <p>An error which occured during the execution of a test case (either setup, call or teardown).</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Error message.</p> <code>traceback</code> <code>Traceback | None</code> <p>Error traceback.</p>"},{"location":"reference/events/steps/test_case_error/#pytest_broadcaster.models.test_case_error.TestCaseError.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message: str\n</code></pre> <p>Error message.</p>"},{"location":"reference/events/steps/test_case_error/#pytest_broadcaster.models.test_case_error.TestCaseError.traceback","title":"traceback  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>traceback: Traceback | None = None\n</code></pre> <p>Error traceback.</p>"},{"location":"reference/events/steps/test_case_setup/","title":"Test Case Setup","text":"<p>JSON Schema</p> <p>https://charbonnierg.github.io/pytest-broadcaster/latest/schemas/test_case_setup.json</p>"},{"location":"reference/events/steps/test_case_setup/#pytest_broadcaster.models.test_case_setup.TestCaseSetup","title":"TestCaseSetup  <code>dataclass</code>","text":"<pre><code>TestCaseSetup(\n    session_id: str,\n    node_id: str,\n    start_timestamp: str,\n    stop_timestamp: str,\n    duration: float,\n    outcome: Outcome,\n    event: str = \"case_setup\",\n    error: TestCaseError | None = None,\n)\n</code></pre> <p>Pytest Test Case Setup.</p> <p>Attributes:</p> Name Type Description <code>duration</code> <code>float</code> <p>Duration of the setup step in seconds.</p> <code>error</code> <code>TestCaseError | None</code> <p>Error details if the setup step failed.</p> <code>event</code> <code>str</code> <p>The event type. Always set to 'case_setup'.</p> <code>node_id</code> <code>str</code> <p>The node ID of the test case.</p> <code>outcome</code> <code>Outcome</code> <p>Outcome of the setup step.</p> <code>session_id</code> <code>str</code> <p>The unique if of this test session used to aggregate events together.</p> <code>start_timestamp</code> <code>str</code> <p>Start time of the setup step in ISO 8601 format.</p> <code>stop_timestamp</code> <code>str</code> <p>Stop time of the setup step in ISO 8601 format.</p>"},{"location":"reference/events/steps/test_case_setup/#pytest_broadcaster.models.test_case_setup.TestCaseSetup.duration","title":"duration  <code>instance-attribute</code>","text":"<pre><code>duration: float\n</code></pre> <p>Duration of the setup step in seconds.</p>"},{"location":"reference/events/steps/test_case_setup/#pytest_broadcaster.models.test_case_setup.TestCaseSetup.error","title":"error  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>error: TestCaseError | None = None\n</code></pre> <p>Error details if the setup step failed.</p>"},{"location":"reference/events/steps/test_case_setup/#pytest_broadcaster.models.test_case_setup.TestCaseSetup.event","title":"event  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>event: str = 'case_setup'\n</code></pre> <p>The event type. Always set to 'case_setup'.</p>"},{"location":"reference/events/steps/test_case_setup/#pytest_broadcaster.models.test_case_setup.TestCaseSetup.node_id","title":"node_id  <code>instance-attribute</code>","text":"<pre><code>node_id: str\n</code></pre> <p>The node ID of the test case.</p>"},{"location":"reference/events/steps/test_case_setup/#pytest_broadcaster.models.test_case_setup.TestCaseSetup.outcome","title":"outcome  <code>instance-attribute</code>","text":"<pre><code>outcome: Outcome\n</code></pre> <p>Outcome of the setup step.</p>"},{"location":"reference/events/steps/test_case_setup/#pytest_broadcaster.models.test_case_setup.TestCaseSetup.session_id","title":"session_id  <code>instance-attribute</code>","text":"<pre><code>session_id: str\n</code></pre> <p>The unique if of this test session used to aggregate events together.</p>"},{"location":"reference/events/steps/test_case_setup/#pytest_broadcaster.models.test_case_setup.TestCaseSetup.start_timestamp","title":"start_timestamp  <code>instance-attribute</code>","text":"<pre><code>start_timestamp: str\n</code></pre> <p>Start time of the setup step in ISO 8601 format.</p>"},{"location":"reference/events/steps/test_case_setup/#pytest_broadcaster.models.test_case_setup.TestCaseSetup.stop_timestamp","title":"stop_timestamp  <code>instance-attribute</code>","text":"<pre><code>stop_timestamp: str\n</code></pre> <p>Stop time of the setup step in ISO 8601 format.</p>"},{"location":"reference/events/steps/test_case_teardown/","title":"Test Case Teardown","text":"<p>JSON Schema</p> <p>https://charbonnierg.github.io/pytest-broadcaster/latest/schemas/test_case_teardown.json</p>"},{"location":"reference/events/steps/test_case_teardown/#pytest_broadcaster.models.test_case_teardown.TestCaseTeardown","title":"TestCaseTeardown  <code>dataclass</code>","text":"<pre><code>TestCaseTeardown(\n    session_id: str,\n    node_id: str,\n    start_timestamp: str,\n    stop_timestamp: str,\n    duration: float,\n    outcome: Outcome,\n    event: str = \"case_teardown\",\n    error: TestCaseError | None = None,\n)\n</code></pre> <p>Pytest Test Case Teardown.</p> <p>Attributes:</p> Name Type Description <code>duration</code> <code>float</code> <p>Duration of the teardown step in seconds.</p> <code>error</code> <code>TestCaseError | None</code> <p>Error details if the teardown step failed.</p> <code>event</code> <code>str</code> <p>The event type. Always set to 'case_teardown'.</p> <code>node_id</code> <code>str</code> <p>The node ID of the test case.</p> <code>outcome</code> <code>Outcome</code> <p>Outcome of the teardown step.</p> <code>session_id</code> <code>str</code> <p>The unique if of this test session used to aggregate events together.</p> <code>start_timestamp</code> <code>str</code> <p>Start time of the teardown step in ISO 8601 format.</p> <code>stop_timestamp</code> <code>str</code> <p>Stop time of the teardown step in ISO 8601 format.</p>"},{"location":"reference/events/steps/test_case_teardown/#pytest_broadcaster.models.test_case_teardown.TestCaseTeardown.duration","title":"duration  <code>instance-attribute</code>","text":"<pre><code>duration: float\n</code></pre> <p>Duration of the teardown step in seconds.</p>"},{"location":"reference/events/steps/test_case_teardown/#pytest_broadcaster.models.test_case_teardown.TestCaseTeardown.error","title":"error  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>error: TestCaseError | None = None\n</code></pre> <p>Error details if the teardown step failed.</p>"},{"location":"reference/events/steps/test_case_teardown/#pytest_broadcaster.models.test_case_teardown.TestCaseTeardown.event","title":"event  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>event: str = 'case_teardown'\n</code></pre> <p>The event type. Always set to 'case_teardown'.</p>"},{"location":"reference/events/steps/test_case_teardown/#pytest_broadcaster.models.test_case_teardown.TestCaseTeardown.node_id","title":"node_id  <code>instance-attribute</code>","text":"<pre><code>node_id: str\n</code></pre> <p>The node ID of the test case.</p>"},{"location":"reference/events/steps/test_case_teardown/#pytest_broadcaster.models.test_case_teardown.TestCaseTeardown.outcome","title":"outcome  <code>instance-attribute</code>","text":"<pre><code>outcome: Outcome\n</code></pre> <p>Outcome of the teardown step.</p>"},{"location":"reference/events/steps/test_case_teardown/#pytest_broadcaster.models.test_case_teardown.TestCaseTeardown.session_id","title":"session_id  <code>instance-attribute</code>","text":"<pre><code>session_id: str\n</code></pre> <p>The unique if of this test session used to aggregate events together.</p>"},{"location":"reference/events/steps/test_case_teardown/#pytest_broadcaster.models.test_case_teardown.TestCaseTeardown.start_timestamp","title":"start_timestamp  <code>instance-attribute</code>","text":"<pre><code>start_timestamp: str\n</code></pre> <p>Start time of the teardown step in ISO 8601 format.</p>"},{"location":"reference/events/steps/test_case_teardown/#pytest_broadcaster.models.test_case_teardown.TestCaseTeardown.stop_timestamp","title":"stop_timestamp  <code>instance-attribute</code>","text":"<pre><code>stop_timestamp: str\n</code></pre> <p>Stop time of the teardown step in ISO 8601 format.</p>"},{"location":"reference/hooks/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Add Destination</li> <li>Set Reporter</li> </ul>"},{"location":"reference/hooks/pytest_broadcaster_add_destination/","title":"<code>pytest_broadcaster_add_destination</code>","text":""},{"location":"reference/hooks/pytest_broadcaster_add_destination/#pytest_broadcaster.hooks.pytest_broadcaster_add_destination","title":"pytest_broadcaster_add_destination","text":"<pre><code>pytest_broadcaster_add_destination(\n    add: Callable[[Destination], None],\n) -&gt; None\n</code></pre> <p>Add your own destination.</p> <p>This function is called on plugin initialization.</p> <p>For instance, in <code>conftest.py</code>:</p> <pre><code>from pytest_broadcaster import HTTPWebhook\n\ndef pytest_broadcaster_add_destination(add):\n    add(HTTPWebhook(url=\"https://example.com\"))\n    add(HTTPWebhook(url=\"https://another-example.com\"))\n</code></pre> <p>Then run pytest without any option:</p> <pre><code>pytest\n</code></pre>"},{"location":"reference/hooks/pytest_broadcaster_set_reporter/","title":"<code>pytest_broadcaster_set_reporter</code>","text":""},{"location":"reference/hooks/pytest_broadcaster_set_reporter/#pytest_broadcaster.hooks.pytest_broadcaster_set_reporter","title":"pytest_broadcaster_set_reporter","text":"<pre><code>pytest_broadcaster_set_reporter(\n    _set: Callable[[Reporter], None],\n) -&gt; None\n</code></pre> <p>Set your own reporter.</p> <p>This funciton is called on plugin initialization.</p> <p>For instance, in <code>conftest.py</code>:</p> <pre><code>def pytest_broadcaster_set_reporter(set_reporter):\n    set_reporter(MyReporter())\n</code></pre> <p>Then run pytest without any option:</p> <pre><code>pytest\n</code></pre>"},{"location":"reference/interfaces/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Destination</li> <li>Reporter</li> </ul>"},{"location":"reference/interfaces/destination/","title":"Destination","text":""},{"location":"reference/interfaces/destination/#pytest_broadcaster.interfaces.Destination","title":"Destination","text":"<p>An interface where you can write events and results.</p> <p>Methods:</p> Name Description <code>close</code> <p>Close the destination. No-op by default.</p> <code>open</code> <p>Open the destination. No-op by default.</p> <code>summary</code> <p>Return a summary of the destination.</p> <code>write_event</code> <p>Write an event to the destination.</p> <code>write_result</code> <p>Write the session result to the destination.</p>"},{"location":"reference/interfaces/destination/#pytest_broadcaster.interfaces.Destination.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the destination. No-op by default.</p>"},{"location":"reference/interfaces/destination/#pytest_broadcaster.interfaces.Destination.open","title":"open","text":"<pre><code>open() -&gt; None\n</code></pre> <p>Open the destination. No-op by default.</p>"},{"location":"reference/interfaces/destination/#pytest_broadcaster.interfaces.Destination.summary","title":"summary  <code>abstractmethod</code>","text":"<pre><code>summary() -&gt; str | None\n</code></pre> <p>Return a summary of the destination.</p>"},{"location":"reference/interfaces/destination/#pytest_broadcaster.interfaces.Destination.write_event","title":"write_event  <code>abstractmethod</code>","text":"<pre><code>write_event(event: SessionEvent) -&gt; None\n</code></pre> <p>Write an event to the destination.</p>"},{"location":"reference/interfaces/destination/#pytest_broadcaster.interfaces.Destination.write_result","title":"write_result  <code>abstractmethod</code>","text":"<pre><code>write_result(result: SessionResult) -&gt; None\n</code></pre> <p>Write the session result to the destination.</p>"},{"location":"reference/interfaces/reporter/","title":"Reporter","text":""},{"location":"reference/interfaces/reporter/#pytest_broadcaster.interfaces.Reporter","title":"Reporter","text":"<p>An interface to create events and results.</p> <p>Methods:</p> Name Description <code>make_collect_report</code> <p>Return a collect report event.</p> <code>make_error_message</code> <p>Return an error message event.</p> <code>make_session_end</code> <p>Return a session env event.</p> <code>make_session_result</code> <p>Return the session result, if session is done, else returns None.</p> <code>make_session_start</code> <p>Return a session start event.</p> <code>make_test_case_end</code> <p>Return a test case end event.</p> <code>make_test_case_step</code> <p>Return a test case step event.</p> <code>make_warning_message</code> <p>Return a warning message event.</p>"},{"location":"reference/interfaces/reporter/#pytest_broadcaster.interfaces.Reporter.make_collect_report","title":"make_collect_report  <code>abstractmethod</code>","text":"<pre><code>make_collect_report(report: CollectReport) -&gt; CollectReport\n</code></pre> <p>Return a collect report event.</p>"},{"location":"reference/interfaces/reporter/#pytest_broadcaster.interfaces.Reporter.make_error_message","title":"make_error_message  <code>abstractmethod</code>","text":"<pre><code>make_error_message(\n    report: CollectReport, call: CallInfo[Any]\n) -&gt; ErrorMessage\n</code></pre> <p>Return an error message event.</p>"},{"location":"reference/interfaces/reporter/#pytest_broadcaster.interfaces.Reporter.make_session_end","title":"make_session_end  <code>abstractmethod</code>","text":"<pre><code>make_session_end(exit_status: int) -&gt; SessionEnd\n</code></pre> <p>Return a session env event.</p>"},{"location":"reference/interfaces/reporter/#pytest_broadcaster.interfaces.Reporter.make_session_result","title":"make_session_result  <code>abstractmethod</code>","text":"<pre><code>make_session_result() -&gt; SessionResult | None\n</code></pre> <p>Return the session result, if session is done, else returns None.</p>"},{"location":"reference/interfaces/reporter/#pytest_broadcaster.interfaces.Reporter.make_session_start","title":"make_session_start  <code>abstractmethod</code>","text":"<pre><code>make_session_start() -&gt; SessionStart\n</code></pre> <p>Return a session start event.</p>"},{"location":"reference/interfaces/reporter/#pytest_broadcaster.interfaces.Reporter.make_test_case_end","title":"make_test_case_end  <code>abstractmethod</code>","text":"<pre><code>make_test_case_end(node_id: str) -&gt; TestCaseEnd\n</code></pre> <p>Return a test case end event.</p>"},{"location":"reference/interfaces/reporter/#pytest_broadcaster.interfaces.Reporter.make_test_case_step","title":"make_test_case_step  <code>abstractmethod</code>","text":"<pre><code>make_test_case_step(\n    report: TestReport,\n) -&gt; TestCaseCall | TestCaseSetup | TestCaseTeardown\n</code></pre> <p>Return a test case step event.</p>"},{"location":"reference/interfaces/reporter/#pytest_broadcaster.interfaces.Reporter.make_warning_message","title":"make_warning_message  <code>abstractmethod</code>","text":"<pre><code>make_warning_message(\n    warning_message: WarningMessage,\n    when: Literal[\"config\", \"collect\", \"runtest\"],\n    nodeid: str,\n) -&gt; WarningMessage\n</code></pre> <p>Return a warning message event.</p>"},{"location":"reference/result/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Session Result<ul> <li>Test Case Report</li> </ul> </li> </ul>"},{"location":"reference/result/session_result/","title":"Session Result","text":""},{"location":"reference/result/session_result/#pytest_broadcaster.models.session_result.SessionResult","title":"SessionResult  <code>dataclass</code>","text":"<pre><code>SessionResult(\n    session_id: str,\n    start_timestamp: str,\n    stop_timestamp: str,\n    python: PythonDistribution,\n    pytest_version: str,\n    plugin_version: str,\n    exit_status: int,\n    errors: list[ErrorMessage],\n    warnings: list[WarningMessage],\n    collect_reports: list[CollectReport],\n    test_reports: list[TestCaseReport],\n    project: Project | None = None,\n)\n</code></pre> <p>Result of a pytest session, including collect reports and test reports.</p> <p>Attributes:</p> Name Type Description <code>collect_reports</code> <code>list[CollectReport]</code> <p>Collect reports generated during the session.</p> <code>errors</code> <code>list[ErrorMessage]</code> <p>Errors generated during the session.</p> <code>exit_status</code> <code>int</code> <p>The exit status of the pytest run. 0 indicates success, non-zero indicates failure.</p> <code>plugin_version</code> <code>str</code> <p>The version of the plugin that generated the report.</p> <code>project</code> <code>Project | None</code> <p>The project that is being tested.</p> <code>pytest_version</code> <code>str</code> <p>The version of pytest that generated the report.</p> <code>python</code> <code>PythonDistribution</code> <p>The Python distribution that ran the tests.</p> <code>session_id</code> <code>str</code> <p>The unique if of this test session.</p> <code>start_timestamp</code> <code>str</code> <p>The start time of the pytest session in ISO 8601 format.</p> <code>stop_timestamp</code> <code>str</code> <p>The stop time of the pytest session in ISO 8601 format.</p> <code>test_reports</code> <code>list[TestCaseReport]</code> <p>Test reports generated during the session.</p> <code>warnings</code> <code>list[WarningMessage]</code> <p>Warnings generated during the session.</p>"},{"location":"reference/result/session_result/#pytest_broadcaster.models.session_result.SessionResult.collect_reports","title":"collect_reports  <code>instance-attribute</code>","text":"<pre><code>collect_reports: list[CollectReport]\n</code></pre> <p>Collect reports generated during the session.</p>"},{"location":"reference/result/session_result/#pytest_broadcaster.models.session_result.SessionResult.errors","title":"errors  <code>instance-attribute</code>","text":"<pre><code>errors: list[ErrorMessage]\n</code></pre> <p>Errors generated during the session.</p>"},{"location":"reference/result/session_result/#pytest_broadcaster.models.session_result.SessionResult.exit_status","title":"exit_status  <code>instance-attribute</code>","text":"<pre><code>exit_status: int\n</code></pre> <p>The exit status of the pytest run. 0 indicates success, non-zero indicates failure.</p>"},{"location":"reference/result/session_result/#pytest_broadcaster.models.session_result.SessionResult.plugin_version","title":"plugin_version  <code>instance-attribute</code>","text":"<pre><code>plugin_version: str\n</code></pre> <p>The version of the plugin that generated the report.</p>"},{"location":"reference/result/session_result/#pytest_broadcaster.models.session_result.SessionResult.project","title":"project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>project: Project | None = None\n</code></pre> <p>The project that is being tested.</p>"},{"location":"reference/result/session_result/#pytest_broadcaster.models.session_result.SessionResult.pytest_version","title":"pytest_version  <code>instance-attribute</code>","text":"<pre><code>pytest_version: str\n</code></pre> <p>The version of pytest that generated the report.</p>"},{"location":"reference/result/session_result/#pytest_broadcaster.models.session_result.SessionResult.python","title":"python  <code>instance-attribute</code>","text":"<pre><code>python: PythonDistribution\n</code></pre> <p>The Python distribution that ran the tests.</p>"},{"location":"reference/result/session_result/#pytest_broadcaster.models.session_result.SessionResult.session_id","title":"session_id  <code>instance-attribute</code>","text":"<pre><code>session_id: str\n</code></pre> <p>The unique if of this test session.</p>"},{"location":"reference/result/session_result/#pytest_broadcaster.models.session_result.SessionResult.start_timestamp","title":"start_timestamp  <code>instance-attribute</code>","text":"<pre><code>start_timestamp: str\n</code></pre> <p>The start time of the pytest session in ISO 8601 format.</p>"},{"location":"reference/result/session_result/#pytest_broadcaster.models.session_result.SessionResult.stop_timestamp","title":"stop_timestamp  <code>instance-attribute</code>","text":"<pre><code>stop_timestamp: str\n</code></pre> <p>The stop time of the pytest session in ISO 8601 format.</p>"},{"location":"reference/result/session_result/#pytest_broadcaster.models.session_result.SessionResult.test_reports","title":"test_reports  <code>instance-attribute</code>","text":"<pre><code>test_reports: list[TestCaseReport]\n</code></pre> <p>Test reports generated during the session.</p>"},{"location":"reference/result/session_result/#pytest_broadcaster.models.session_result.SessionResult.warnings","title":"warnings  <code>instance-attribute</code>","text":"<pre><code>warnings: list[WarningMessage]\n</code></pre> <p>Warnings generated during the session.</p>"},{"location":"reference/result/session_result/test_case_report/","title":"Test Case Report","text":""},{"location":"reference/result/session_result/test_case_report/#pytest_broadcaster.models.test_case_report.TestCaseReport","title":"TestCaseReport  <code>dataclass</code>","text":"<pre><code>TestCaseReport(\n    node_id: str,\n    outcome: Outcome,\n    duration: float,\n    setup: TestCaseSetup,\n    teardown: TestCaseTeardown,\n    finished: TestCaseEnd,\n    call: TestCaseCall | None = None,\n)\n</code></pre> <p>Report for a single test case.</p> <p>Attributes:</p> Name Type Description <code>call</code> <code>TestCaseCall | None</code> <p>Call step of the test case (optional).</p> <code>duration</code> <code>float</code> <p>Duration of the test case in seconds (including setup and teardown).</p> <code>finished</code> <code>TestCaseEnd</code> <p>View of the test case after it has finished.</p> <code>node_id</code> <code>str</code> <p>The node ID of the test case.</p> <code>outcome</code> <code>Outcome</code> <p>Outcome of the test case.</p> <code>setup</code> <code>TestCaseSetup</code> <p>Setup step of the test case.</p> <code>teardown</code> <code>TestCaseTeardown</code> <p>Teardown step of the test case.</p>"},{"location":"reference/result/session_result/test_case_report/#pytest_broadcaster.models.test_case_report.TestCaseReport.call","title":"call  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>call: TestCaseCall | None = None\n</code></pre> <p>Call step of the test case (optional).</p>"},{"location":"reference/result/session_result/test_case_report/#pytest_broadcaster.models.test_case_report.TestCaseReport.duration","title":"duration  <code>instance-attribute</code>","text":"<pre><code>duration: float\n</code></pre> <p>Duration of the test case in seconds (including setup and teardown).</p>"},{"location":"reference/result/session_result/test_case_report/#pytest_broadcaster.models.test_case_report.TestCaseReport.finished","title":"finished  <code>instance-attribute</code>","text":"<pre><code>finished: TestCaseEnd\n</code></pre> <p>View of the test case after it has finished.</p>"},{"location":"reference/result/session_result/test_case_report/#pytest_broadcaster.models.test_case_report.TestCaseReport.node_id","title":"node_id  <code>instance-attribute</code>","text":"<pre><code>node_id: str\n</code></pre> <p>The node ID of the test case.</p>"},{"location":"reference/result/session_result/test_case_report/#pytest_broadcaster.models.test_case_report.TestCaseReport.outcome","title":"outcome  <code>instance-attribute</code>","text":"<pre><code>outcome: Outcome\n</code></pre> <p>Outcome of the test case.</p>"},{"location":"reference/result/session_result/test_case_report/#pytest_broadcaster.models.test_case_report.TestCaseReport.setup","title":"setup  <code>instance-attribute</code>","text":"<pre><code>setup: TestCaseSetup\n</code></pre> <p>Setup step of the test case.</p>"},{"location":"reference/result/session_result/test_case_report/#pytest_broadcaster.models.test_case_report.TestCaseReport.teardown","title":"teardown  <code>instance-attribute</code>","text":"<pre><code>teardown: TestCaseTeardown\n</code></pre> <p>Teardown step of the test case.</p>"},{"location":"schemas/","title":"JSON Schemas","text":"<p>The table below contains the JSON schemas used in the project:</p> Schema Description URL CollectReport A report of collected items. https://charbonnierg.github.io/pytest-broadcaster/v0.10.0/schemas/collect_report.json Location Location in code source. https://charbonnierg.github.io/pytest-broadcaster/v0.10.0/schemas/location.json TestCaseSetup Pytest Test Case Setup https://charbonnierg.github.io/pytest-broadcaster/v0.10.0/schemas/test_case_setup.json TestCaseReport Report for a single test case. https://charbonnierg.github.io/pytest-broadcaster/v0.10.0/schemas/test_case_report.json TestCaseEnd Event emitted when a test case is finished (meaning all of setup, call and teardown steps are finished). https://charbonnierg.github.io/pytest-broadcaster/v0.10.0/schemas/test_case_end.json TestModule A collected test module (test file). https://charbonnierg.github.io/pytest-broadcaster/v0.10.0/schemas/test_module.json TestCaseCall Event emitted when a call step has been executed in a test case. https://charbonnierg.github.io/pytest-broadcaster/v0.10.0/schemas/test_case_call.json Outcome The outcome of a case step or a whole test case. https://charbonnierg.github.io/pytest-broadcaster/v0.10.0/schemas/outcome.json SessionEvent An event within a pytest session. https://charbonnierg.github.io/pytest-broadcaster/v0.10.0/schemas/session_event.json ErrorMessage An error message. https://charbonnierg.github.io/pytest-broadcaster/v0.10.0/schemas/error_message.json Traceback An error traceback. https://charbonnierg.github.io/pytest-broadcaster/v0.10.0/schemas/traceback.json SessionEnd Event emitted when the session is finished. https://charbonnierg.github.io/pytest-broadcaster/v0.10.0/schemas/session_end.json TestCase A collected test case. https://charbonnierg.github.io/pytest-broadcaster/v0.10.0/schemas/test_case.json PythonDistribution Metadata about the python interpreter being used to run the tests. https://charbonnierg.github.io/pytest-broadcaster/v0.10.0/schemas/python_distribution.json SessionStart Event emitted when the test session is started. https://charbonnierg.github.io/pytest-broadcaster/v0.10.0/schemas/session_start.json WarningMessage A warning message. https://charbonnierg.github.io/pytest-broadcaster/v0.10.0/schemas/warning_message.json TestDirectory A collected test directory. https://charbonnierg.github.io/pytest-broadcaster/v0.10.0/schemas/test_directory.json TestCaseTeardown Pytest Test Case Teardown https://charbonnierg.github.io/pytest-broadcaster/v0.10.0/schemas/test_case_teardown.json SessionResult Result of a pytest session, including collect reports and test reports. https://charbonnierg.github.io/pytest-broadcaster/v0.10.0/schemas/session_result.json TestCaseError An error which occured during the execution of a test case (either setup, call or teardown). https://charbonnierg.github.io/pytest-broadcaster/v0.10.0/schemas/test_case_error.json TestSuite A collected test suite (test class). https://charbonnierg.github.io/pytest-broadcaster/v0.10.0/schemas/test_suite.json Project Metadata about the project for which tests are run. https://charbonnierg.github.io/pytest-broadcaster/v0.10.0/schemas/project.json"},{"location":"usage/SUMMARY/","title":"SUMMARY","text":"<ul> <li>JSON File</li> <li>JSON Lines File</li> <li>HTTP Webhook</li> <li>HTTP Webhook (Stream)</li> </ul>"},{"location":"usage/http_webhook/","title":"Sending JSON to a Webhook","text":"<p>To publish a JSON report over HTTP, you can use the <code>--collect-url</code> option with a URL. This will send a POST request with the session result.</p> Option Description <code>--collect-url</code> Send a JSON report file with the session result to a <code>HTTP</code> webhook using a <code>POST</code> request. pytest --collect-url=http://localhost:8000 <p>The <code>POST</code> request is sent on session exit, after all tests have been collected and run.</p>"},{"location":"usage/http_webhook_stream/","title":"Streaming JSON to a Webhook","text":"<p>To publish a JSON Lines log stream over HTTP, you can use the <code>--collect-log-url</code> option with a URL. This will send a POST request for each session event.</p> Option Description <code>--collect-log-url</code> Send session events to <code>HTTP</code> webhook using a <code>POST</code> requests. pytest --collect-log-url=http://localhost:8000 <p>A <code>POST</code> request is sent for each event as it occurs during the session.</p>"},{"location":"usage/json_file/","title":"Generating a JSON report","text":"<p>To generate a JSON report, you can use the <code>--collect-report</code> option with a filename. This will output a JSON file with the session result.</p> Option Description <code>--collect-report</code> Output a JSON report file with the session result. pytest --collect-report=report.json <p>The report will be written on session exit, after all tests have been collected and run.</p>"},{"location":"usage/json_lines/","title":"Generating a JSON Lines log stream","text":"<p>To generate a JSON Lines log stream, you can use the <code>--collect-log</code> option with a filename. This will output a JSON Lines stream with the session events.</p> Option Description <code>--collect-log</code> Output session events to JSON Lines file. pytest --collect-log=events.log <p>The log stream is written as events occur during the session.</p>"}]}